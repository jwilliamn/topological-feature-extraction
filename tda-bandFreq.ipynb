{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ripser import ripser, Rips\n",
    "from persim import plot_diagrams, persistent_entropy, sliced_wasserstein, bottleneck, bottleneck_matching\n",
    "from gtda.time_series import SingleTakensEmbedding\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceEntropy, BettiCurve, PersistenceImage\n",
    "from gtda.diagrams import Amplitude, NumberOfPoints\n",
    "\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis for homological dimension 1\n",
    "hdim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('preproc_data_/preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = data[data['target']=='healthy'].sample(n=76, random_state=1)\n",
    "trauma = data[data['target']=='trauma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = healthy.append(trauma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_embedding_dimension = 10\n",
    "max_time_delay = 10\n",
    "stride = 3\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", time_delay=max_time_delay, dimension=max_embedding_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_embedder(embedder: SingleTakensEmbedding, y: np.ndarray, verbose: bool=True) -> np.ndarray:\n",
    "    \"\"\"Fits a Takens embedder and displays optimal search parameters.\"\"\"\n",
    "    y_embedded = embedder.fit_transform(y)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Shape of embedded time series: {y_embedded.shape}\")\n",
    "        print(\n",
    "            f\"Optimal embedding dimension is {embedder.dimension_} and time delay is {embedder.time_delay_}\"\n",
    "        )\n",
    "\n",
    "    return y_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 104 seconds\n"
     ]
    }
   ],
   "source": [
    "file = data['fn']\n",
    "\n",
    "point_cloud = []\n",
    "\n",
    "tic = time.time()\n",
    "for i, f in enumerate(file):\n",
    "    file_path = join('preproc_data_', f)\n",
    "    signal = pd.read_csv(file_path)\n",
    "    \n",
    "    embedding = []\n",
    "    \n",
    "    for j in range(1, signal.shape[1]):\n",
    "        feati_embedded = fit_embedder(embedder, signal.iloc[:,j], False)\n",
    "        feati_embedded = feati_embedded[None, :, :]\n",
    "        embedding.append(feati_embedded)\n",
    "    \n",
    "    point_cloud.append(embedding)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "timelapse = toc - tic\n",
    "print(\"Elapsed Time: %.3g seconds\" % (timelapse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistent_homology(fembedding, hdim):\n",
    "    homology_dimensions = [hdim]\n",
    "    \n",
    "    persistence = VietorisRipsPersistence(homology_dimensions=homology_dimensions, n_jobs=-1)\n",
    "    diagram = persistence.fit_transform(fembedding)\n",
    "    \n",
    "    return diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject:  0\n",
      "Processing subject:  1\n",
      "Processing subject:  2\n",
      "Processing subject:  3\n",
      "Processing subject:  4\n",
      "Processing subject:  5\n",
      "Processing subject:  6\n",
      "Processing subject:  7\n",
      "Processing subject:  8\n",
      "Processing subject:  9\n",
      "Processing subject:  10\n",
      "Processing subject:  11\n",
      "Processing subject:  12\n",
      "Processing subject:  13\n",
      "Processing subject:  14\n",
      "Processing subject:  15\n",
      "Processing subject:  16\n",
      "Processing subject:  17\n",
      "Processing subject:  18\n",
      "Processing subject:  19\n",
      "Processing subject:  20\n",
      "Processing subject:  21\n",
      "Processing subject:  22\n",
      "Processing subject:  23\n",
      "Processing subject:  24\n",
      "Processing subject:  25\n",
      "Processing subject:  26\n",
      "Processing subject:  27\n",
      "Processing subject:  28\n",
      "Processing subject:  29\n",
      "Processing subject:  30\n",
      "Processing subject:  31\n",
      "Processing subject:  32\n",
      "Processing subject:  33\n",
      "Processing subject:  34\n",
      "Processing subject:  35\n",
      "Processing subject:  36\n",
      "Processing subject:  37\n",
      "Processing subject:  38\n",
      "Processing subject:  39\n",
      "Processing subject:  40\n",
      "Processing subject:  41\n",
      "Processing subject:  42\n",
      "Processing subject:  43\n",
      "Processing subject:  44\n",
      "Processing subject:  45\n",
      "Processing subject:  46\n",
      "Processing subject:  47\n",
      "Processing subject:  48\n",
      "Processing subject:  49\n",
      "Processing subject:  50\n",
      "Processing subject:  51\n",
      "Processing subject:  52\n",
      "Processing subject:  53\n",
      "Processing subject:  54\n",
      "Processing subject:  55\n",
      "Processing subject:  56\n",
      "Processing subject:  57\n",
      "Processing subject:  58\n",
      "Processing subject:  59\n",
      "Processing subject:  60\n",
      "Processing subject:  61\n",
      "Processing subject:  62\n",
      "Processing subject:  63\n",
      "Processing subject:  64\n",
      "Processing subject:  65\n",
      "Processing subject:  66\n",
      "Processing subject:  67\n",
      "Processing subject:  68\n",
      "Processing subject:  69\n",
      "Processing subject:  70\n",
      "Processing subject:  71\n",
      "Processing subject:  72\n",
      "Processing subject:  73\n",
      "Processing subject:  74\n",
      "Processing subject:  75\n",
      "Processing subject:  76\n",
      "Processing subject:  77\n",
      "Processing subject:  78\n",
      "Processing subject:  79\n",
      "Processing subject:  80\n",
      "Processing subject:  81\n",
      "Processing subject:  82\n",
      "Processing subject:  83\n",
      "Processing subject:  84\n",
      "Processing subject:  85\n",
      "Processing subject:  86\n",
      "Processing subject:  87\n",
      "Processing subject:  88\n",
      "Processing subject:  89\n",
      "Processing subject:  90\n",
      "Processing subject:  91\n",
      "Processing subject:  92\n",
      "Processing subject:  93\n",
      "Processing subject:  94\n",
      "Processing subject:  95\n",
      "Processing subject:  96\n",
      "Processing subject:  97\n",
      "Processing subject:  98\n",
      "Processing subject:  99\n",
      "Processing subject:  100\n",
      "Processing subject:  101\n",
      "Processing subject:  102\n",
      "Processing subject:  103\n",
      "Processing subject:  104\n",
      "Processing subject:  105\n",
      "Processing subject:  106\n",
      "Processing subject:  107\n",
      "Processing subject:  108\n",
      "Processing subject:  109\n",
      "Processing subject:  110\n",
      "Processing subject:  111\n",
      "Processing subject:  112\n",
      "Processing subject:  113\n",
      "Processing subject:  114\n",
      "Processing subject:  115\n",
      "Processing subject:  116\n",
      "Processing subject:  117\n",
      "Processing subject:  118\n",
      "Processing subject:  119\n",
      "Processing subject:  120\n",
      "Processing subject:  121\n",
      "Processing subject:  122\n",
      "Processing subject:  123\n",
      "Processing subject:  124\n",
      "Processing subject:  125\n",
      "Processing subject:  126\n",
      "Processing subject:  127\n",
      "Processing subject:  128\n",
      "Processing subject:  129\n",
      "Processing subject:  130\n",
      "Processing subject:  131\n",
      "Processing subject:  132\n",
      "Processing subject:  133\n",
      "Processing subject:  134\n",
      "Processing subject:  135\n",
      "Processing subject:  136\n",
      "Processing subject:  137\n",
      "Processing subject:  138\n",
      "Processing subject:  139\n",
      "Processing subject:  140\n",
      "Processing subject:  141\n",
      "Processing subject:  142\n",
      "Processing subject:  143\n",
      "Processing subject:  144\n",
      "Processing subject:  145\n",
      "Processing subject:  146\n",
      "Processing subject:  147\n",
      "Processing subject:  148\n",
      "Processing subject:  149\n",
      "Processing subject:  150\n",
      "Processing subject:  151\n",
      "Elapsed Time: 468 seconds\n"
     ]
    }
   ],
   "source": [
    "n = len(point_cloud) # number of subjects\n",
    "m = len(point_cloud[0]) # number of channels (original features)\n",
    "\n",
    "hdimension = hdim\n",
    "diagrams = []\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n):\n",
    "    \n",
    "    print(\"Processing subject: \", i)\n",
    "    diagram_ = []\n",
    "    \n",
    "    for j in range(m):\n",
    "        diagram_.append(persistent_homology(point_cloud[i][j], hdimension))\n",
    "    \n",
    "    diagrams.append(diagram_)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "timelapse = toc - tic\n",
    "print(\"Elapsed Time: %.3g seconds\" % (timelapse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(diagram):\n",
    "    \n",
    "    # Metrics to calculate amplitudes\n",
    "    metrics = [\n",
    "        {\"metric\": metric}\n",
    "        for metric in [\"bottleneck\", \"wasserstein\", \"landscape\", \"persistence_image\"]\n",
    "    ]\n",
    "\n",
    "    # Generation of topological features\n",
    "    topological_feature = make_union(\n",
    "        PersistenceEntropy(normalize=True),\n",
    "        NumberOfPoints(n_jobs=-1),\n",
    "        *[Amplitude(**metric, n_jobs=-1) for metric in metrics]\n",
    "    )\n",
    "\n",
    "    feature_i = topological_feature.fit_transform(diagram)\n",
    "    \n",
    "    return feature_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature extraction from 1 Homological dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for subject:  0\n",
      "Extracting features for subject:  1\n",
      "Extracting features for subject:  2\n",
      "Extracting features for subject:  3\n",
      "Extracting features for subject:  4\n",
      "Extracting features for subject:  5\n",
      "Extracting features for subject:  6\n",
      "Extracting features for subject:  7\n",
      "Extracting features for subject:  8\n",
      "Extracting features for subject:  9\n",
      "Extracting features for subject:  10\n",
      "Extracting features for subject:  11\n",
      "Extracting features for subject:  12\n",
      "Extracting features for subject:  13\n",
      "Extracting features for subject:  14\n",
      "Extracting features for subject:  15\n",
      "Extracting features for subject:  16\n",
      "Extracting features for subject:  17\n",
      "Extracting features for subject:  18\n",
      "Extracting features for subject:  19\n",
      "Extracting features for subject:  20\n",
      "Extracting features for subject:  21\n",
      "Extracting features for subject:  22\n",
      "Extracting features for subject:  23\n",
      "Extracting features for subject:  24\n",
      "Extracting features for subject:  25\n",
      "Extracting features for subject:  26\n",
      "Extracting features for subject:  27\n",
      "Extracting features for subject:  28\n",
      "Extracting features for subject:  29\n",
      "Extracting features for subject:  30\n",
      "Extracting features for subject:  31\n",
      "Extracting features for subject:  32\n",
      "Extracting features for subject:  33\n",
      "Extracting features for subject:  34\n",
      "Extracting features for subject:  35\n",
      "Extracting features for subject:  36\n",
      "Extracting features for subject:  37\n",
      "Extracting features for subject:  38\n",
      "Extracting features for subject:  39\n",
      "Extracting features for subject:  40\n",
      "Extracting features for subject:  41\n",
      "Extracting features for subject:  42\n",
      "Extracting features for subject:  43\n",
      "Extracting features for subject:  44\n",
      "Extracting features for subject:  45\n",
      "Extracting features for subject:  46\n",
      "Extracting features for subject:  47\n",
      "Extracting features for subject:  48\n",
      "Extracting features for subject:  49\n",
      "Extracting features for subject:  50\n",
      "Extracting features for subject:  51\n",
      "Extracting features for subject:  52\n",
      "Extracting features for subject:  53\n",
      "Extracting features for subject:  54\n",
      "Extracting features for subject:  55\n",
      "Extracting features for subject:  56\n",
      "Extracting features for subject:  57\n",
      "Extracting features for subject:  58\n",
      "Extracting features for subject:  59\n",
      "Extracting features for subject:  60\n",
      "Extracting features for subject:  61\n",
      "Extracting features for subject:  62\n",
      "Extracting features for subject:  63\n",
      "Extracting features for subject:  64\n",
      "Extracting features for subject:  65\n",
      "Extracting features for subject:  66\n",
      "Extracting features for subject:  67\n",
      "Extracting features for subject:  68\n",
      "Extracting features for subject:  69\n",
      "Extracting features for subject:  70\n",
      "Extracting features for subject:  71\n",
      "Extracting features for subject:  72\n",
      "Extracting features for subject:  73\n",
      "Extracting features for subject:  74\n",
      "Extracting features for subject:  75\n",
      "Extracting features for subject:  76\n",
      "Extracting features for subject:  77\n",
      "Extracting features for subject:  78\n",
      "Extracting features for subject:  79\n",
      "Extracting features for subject:  80\n",
      "Extracting features for subject:  81\n",
      "Extracting features for subject:  82\n",
      "Extracting features for subject:  83\n",
      "Extracting features for subject:  84\n",
      "Extracting features for subject:  85\n",
      "Extracting features for subject:  86\n",
      "Extracting features for subject:  87\n",
      "Extracting features for subject:  88\n",
      "Extracting features for subject:  89\n",
      "Extracting features for subject:  90\n",
      "Extracting features for subject:  91\n",
      "Extracting features for subject:  92\n",
      "Extracting features for subject:  93\n",
      "Extracting features for subject:  94\n",
      "Extracting features for subject:  95\n",
      "Extracting features for subject:  96\n",
      "Extracting features for subject:  97\n",
      "Extracting features for subject:  98\n",
      "Extracting features for subject:  99\n",
      "Extracting features for subject:  100\n",
      "Extracting features for subject:  101\n",
      "Extracting features for subject:  102\n",
      "Extracting features for subject:  103\n",
      "Extracting features for subject:  104\n",
      "Extracting features for subject:  105\n",
      "Extracting features for subject:  106\n",
      "Extracting features for subject:  107\n",
      "Extracting features for subject:  108\n",
      "Extracting features for subject:  109\n",
      "Extracting features for subject:  110\n",
      "Extracting features for subject:  111\n",
      "Extracting features for subject:  112\n",
      "Extracting features for subject:  113\n",
      "Extracting features for subject:  114\n",
      "Extracting features for subject:  115\n",
      "Extracting features for subject:  116\n",
      "Extracting features for subject:  117\n",
      "Extracting features for subject:  118\n",
      "Extracting features for subject:  119\n",
      "Extracting features for subject:  120\n",
      "Extracting features for subject:  121\n",
      "Extracting features for subject:  122\n",
      "Extracting features for subject:  123\n",
      "Extracting features for subject:  124\n",
      "Extracting features for subject:  125\n",
      "Extracting features for subject:  126\n",
      "Extracting features for subject:  127\n",
      "Extracting features for subject:  128\n",
      "Extracting features for subject:  129\n",
      "Extracting features for subject:  130\n",
      "Extracting features for subject:  131\n",
      "Extracting features for subject:  132\n",
      "Extracting features for subject:  133\n",
      "Extracting features for subject:  134\n",
      "Extracting features for subject:  135\n",
      "Extracting features for subject:  136\n",
      "Extracting features for subject:  137\n",
      "Extracting features for subject:  138\n",
      "Extracting features for subject:  139\n",
      "Extracting features for subject:  140\n",
      "Extracting features for subject:  141\n",
      "Extracting features for subject:  142\n",
      "Extracting features for subject:  143\n",
      "Extracting features for subject:  144\n",
      "Extracting features for subject:  145\n",
      "Extracting features for subject:  146\n",
      "Extracting features for subject:  147\n",
      "Extracting features for subject:  148\n",
      "Extracting features for subject:  149\n",
      "Extracting features for subject:  150\n",
      "Extracting features for subject:  151\n",
      "Elapsed Time: 7.34e+03 seconds\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n):\n",
    "    \n",
    "    print(\"Extracting features for subject: \", i)\n",
    "    feature_ = extract_features(diagrams[i][0])\n",
    "    \n",
    "    for j in range(1, m):\n",
    "        feature_ = np.append(feature_, extract_features(diagrams[i][j]))\n",
    "    \n",
    "    features.append(feature_)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "timelapse = toc - tic\n",
    "print(\"Elapsed Time: %.3g seconds\" % (timelapse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.057422</td>\n",
       "      <td>516.0</td>\n",
       "      <td>1.078493</td>\n",
       "      <td>7.609457</td>\n",
       "      <td>1.625817</td>\n",
       "      <td>240.086791</td>\n",
       "      <td>1.040205</td>\n",
       "      <td>510.0</td>\n",
       "      <td>1.458287</td>\n",
       "      <td>8.327879</td>\n",
       "      <td>...</td>\n",
       "      <td>1.759852</td>\n",
       "      <td>9.577967</td>\n",
       "      <td>3.196466</td>\n",
       "      <td>210.555888</td>\n",
       "      <td>0.991431</td>\n",
       "      <td>492.0</td>\n",
       "      <td>1.875970</td>\n",
       "      <td>10.882537</td>\n",
       "      <td>3.736711</td>\n",
       "      <td>182.475285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>503.0</td>\n",
       "      <td>1.944156</td>\n",
       "      <td>14.714389</td>\n",
       "      <td>4.243747</td>\n",
       "      <td>170.602065</td>\n",
       "      <td>1.018074</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1.396510</td>\n",
       "      <td>9.424052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957242</td>\n",
       "      <td>7.267961</td>\n",
       "      <td>1.381102</td>\n",
       "      <td>257.864981</td>\n",
       "      <td>1.066423</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.016243</td>\n",
       "      <td>7.024251</td>\n",
       "      <td>1.655713</td>\n",
       "      <td>235.886002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.001907</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2.265818</td>\n",
       "      <td>9.232568</td>\n",
       "      <td>4.269111</td>\n",
       "      <td>150.670842</td>\n",
       "      <td>1.036141</td>\n",
       "      <td>401.0</td>\n",
       "      <td>1.721094</td>\n",
       "      <td>7.744508</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241644</td>\n",
       "      <td>4.538302</td>\n",
       "      <td>1.427395</td>\n",
       "      <td>358.169170</td>\n",
       "      <td>1.117105</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.178067</td>\n",
       "      <td>5.557385</td>\n",
       "      <td>1.762974</td>\n",
       "      <td>264.469598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.892047</td>\n",
       "      <td>442.0</td>\n",
       "      <td>3.056262</td>\n",
       "      <td>19.356665</td>\n",
       "      <td>7.804968</td>\n",
       "      <td>98.733253</td>\n",
       "      <td>0.979256</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1.833882</td>\n",
       "      <td>10.771632</td>\n",
       "      <td>...</td>\n",
       "      <td>6.829617</td>\n",
       "      <td>36.005833</td>\n",
       "      <td>20.342606</td>\n",
       "      <td>68.578706</td>\n",
       "      <td>0.831882</td>\n",
       "      <td>431.0</td>\n",
       "      <td>7.570616</td>\n",
       "      <td>30.930352</td>\n",
       "      <td>24.124375</td>\n",
       "      <td>69.595830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.005080</td>\n",
       "      <td>538.0</td>\n",
       "      <td>1.528481</td>\n",
       "      <td>10.438202</td>\n",
       "      <td>3.505291</td>\n",
       "      <td>194.476743</td>\n",
       "      <td>1.017692</td>\n",
       "      <td>532.0</td>\n",
       "      <td>1.621855</td>\n",
       "      <td>9.688185</td>\n",
       "      <td>...</td>\n",
       "      <td>2.766696</td>\n",
       "      <td>14.993556</td>\n",
       "      <td>6.180069</td>\n",
       "      <td>101.878752</td>\n",
       "      <td>0.939892</td>\n",
       "      <td>486.0</td>\n",
       "      <td>1.932256</td>\n",
       "      <td>14.908890</td>\n",
       "      <td>4.437211</td>\n",
       "      <td>140.079193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1         2          3         4           5         6    \\\n",
       "0  1.057422  516.0  1.078493   7.609457  1.625817  240.086791  1.040205   \n",
       "1  0.944617  503.0  1.944156  14.714389  4.243747  170.602065  1.018074   \n",
       "2  1.001907  400.0  2.265818   9.232568  4.269111  150.670842  1.036141   \n",
       "3  0.892047  442.0  3.056262  19.356665  7.804968   98.733253  0.979256   \n",
       "4  1.005080  538.0  1.528481  10.438202  3.505291  194.476743  1.017692   \n",
       "\n",
       "     7         8          9    ...       92         93         94   \\\n",
       "0  510.0  1.458287   8.327879  ...  1.759852   9.577967   3.196466   \n",
       "1  496.0  1.396510   9.424052  ...  0.957242   7.267961   1.381102   \n",
       "2  401.0  1.721094   7.744508  ...  1.241644   4.538302   1.427395   \n",
       "3  418.0  1.833882  10.771632  ...  6.829617  36.005833  20.342606   \n",
       "4  532.0  1.621855   9.688185  ...  2.766696  14.993556   6.180069   \n",
       "\n",
       "          95        96     97        98         99         100         101  \n",
       "0  210.555888  0.991431  492.0  1.875970  10.882537   3.736711  182.475285  \n",
       "1  257.864981  1.066423  463.0  1.016243   7.024251   1.655713  235.886002  \n",
       "2  358.169170  1.117105  472.0  1.178067   5.557385   1.762974  264.469598  \n",
       "3   68.578706  0.831882  431.0  7.570616  30.930352  24.124375   69.595830  \n",
       "4  101.878752  0.939892  486.0  1.932256  14.908890   4.437211  140.079193  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Renaming features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels\n",
    "cols = list(signal.columns)\n",
    "cols.remove('time')\n",
    "\n",
    "# Topological features\n",
    "feat = ['perEnt', 'numPoi', 'bottlD', 'wassD', 'landSc', 'perImg']\n",
    "feat = [f + '_h' + str(hdimension) for f in feat]\n",
    "\n",
    "# Renamed features\n",
    "cols = [col + '_' + f for col in cols for f in feat]\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = data['target'].values\n",
    "\n",
    "df.loc[df['target']=='trauma', 'target'] = 1\n",
    "df.loc[df['target']=='healthy', 'target'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'tda_data/datar125Hz_f_1_50Hz_h{hdimension}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('py37': conda)",
   "language": "python",
   "name": "python37464bitpy37conda4882f11bbfdc4f7eb6de48bfaf378a30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
